{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXkBCSlh8Inx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "path = \"/content/drive/My Drive/COMP_551_Machine_Learning/Miniproject3/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4Ms1zKF72CE"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn import feature_extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sys\n",
        "import random as rn\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "\n",
        "#Seed to stabilize outcomes\n",
        "rn.seed(321)\n",
        "np.random.seed(321)\n",
        "torch.manual_seed(321)\n",
        "torch.cuda.manual_seed(321)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js9teBX77585"
      },
      "outputs": [],
      "source": [
        "rawTrain_x = []\n",
        "rawTrain_y = []\n",
        "\n",
        "rawTest_x = []\n",
        "rawTest_y = []\n",
        "\n",
        "# Old way to load files on Kaggle, you can safely ignore this.\n",
        "# for dirname, _, filenames in os.walk(path):\n",
        "#     for filename in filenames:\n",
        "#         if (\"train\" in dirname) and os.path.isfile(os.path.join(dirname, filename)):  \n",
        "#             f = open(os.path.join(dirname, filename), 'r')\n",
        "#             rawTrain_x.append(f.read().lower())\n",
        "#             f.close()\n",
        "#             if (\"pos\" in dirname):\n",
        "#                 rawTrain_y.append(int(1))\n",
        "#             else:\n",
        "#                 rawTrain_y.append(int(0))\n",
        "#             #rawTrain_y.append(int(re.search(\"(?<=_)(.*?)(?=\\.)\",filename).group(1)))\n",
        "            \n",
        "#         if (\"test\" in dirname) and os.path.isfile(os.path.join(dirname, filename)):  \n",
        "#             f = open(os.path.join(dirname, filename), 'r')\n",
        "#             rawTest_x.append(f.read().lower())\n",
        "#             f.close()\n",
        "#             if (\"pos\" in dirname):\n",
        "#                 rawTest_y.append(int(1))\n",
        "#             else:\n",
        "#                 rawTest_y.append(int(0))\n",
        "#             #rawTest_y.append(int(re.search(\"(?<=_)(.*?)(?=\\.)\",filename).group(1)))\n",
        "            \n",
        "# rawTrain_x = np.asarray(rawTrain_x)\n",
        "# rawTrain_y = np.asarray(rawTrain_y)\n",
        "# rawTest_x = np.asarray(rawTest_x)\n",
        "# rawTest_y = np.asarray(rawTest_y)\n",
        "\n",
        "# print(rawTrain_x.shape)\n",
        "# print(rawTrain_y.shape)\n",
        "# print(rawTest_x.shape)\n",
        "# print(rawTest_y.shape)\n",
        "\n",
        "# print(np.max(rawTest_y))\n",
        "# print(np.min(rawTest_y))\n",
        "\n",
        "# Load data using tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "data = tfds.load('imdb_reviews', as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XuRtHriDS1G"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = data['train'], data['test']\n",
        "rawTrain_x = []\n",
        "rawTrain_y = []\n",
        "\n",
        "for sentence, label in train_data:\n",
        "    rawTrain_x.append(str(sentence.numpy()))\n",
        "    rawTrain_y.append(int(str(label.numpy())))\n",
        "\n",
        "for sentence, label in test_data:\n",
        "    rawTest_x.append(str(sentence.numpy()))\n",
        "    rawTest_y.append(int(str(label.numpy())))\n",
        "\n",
        "rawTrain_x = np.asarray(rawTrain_x)\n",
        "rawTrain_y = np.asarray(rawTrain_y)\n",
        "rawTest_x = np.asarray(rawTest_x)\n",
        "rawTest_y = np.asarray(rawTest_y)\n",
        "\n",
        "print(rawTrain_x.shape)\n",
        "print(rawTest_x.shape)\n",
        "\n",
        "print(np.min(rawTest_y))\n",
        "print(np.max(rawTest_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKaQfHnl_vOX"
      },
      "outputs": [],
      "source": [
        "#Save data for later use\n",
        "np.savez('zipData.npz',Train_X=rawTrain_x,Train_Y=rawTrain_y, Test_X=rawTest_x, Test_Y=rawTest_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZG-8AnL_wbB"
      },
      "outputs": [],
      "source": [
        "Data_load = np.load('zipData.npz')\n",
        "rawTrain_x = Data_load['Train_X']\n",
        "rawTrain_y = Data_load['Train_Y']\n",
        "rawTest_x = Data_load['Test_X']\n",
        "rawTest_y = Data_load['Test_Y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy5yOnJ8_xsr"
      },
      "outputs": [],
      "source": [
        "# Countvectorizer is a method to convert text to numerical data\n",
        "# Each input is preprocessed, tokenized, and represented as a sparse matrix\n",
        "# Bag-Of-Words representation: transforms the text into fixed-length vectors\n",
        "# Bag-of-Words: doesnâ€™t take into account the order and the structure of the words, but only number of times a word appears in the sentence\n",
        "\n",
        "# Preprocess Data using NLTK for punctuation, stop words removal, lower case, stemming etc.\n",
        "import nltk # Natural Language Toolkit \n",
        "from nltk.corpus import stopwords, movie_reviews, wordnet\n",
        "from nltk import word_tokenize, wordnet\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "nltk.download(['stopwords', 'movie_reviews', 'punkt', 'wordnet'])\n",
        "\n",
        "# Form Bag of Words Representation using CountVectorizer()\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(rawTrain_x)\n",
        "vector = vectorizer.transform(rawTrain_x)\n",
        "\n",
        "words_raw = np.asarray(sorted(vectorizer.vocabulary_)) # List of vocabulary present in Training Data\n",
        "words_raw = np.char.lower(words_raw)\n",
        "print(words_raw.shape)\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words()\n",
        "words = [w for w in words_raw if w.lower() not in stopwords]\n",
        "print(np.asarray(words).shape)\n",
        "\n",
        "# print(sorted(vectorizer.vocabulary_))\n",
        "# print(vector.shape)\n",
        "\n",
        "# Note: filter \"out of vocabulary\" (OOV) words out of test data before applying the model\n",
        "# Also can filter out rare words from the training set, replace using an \"UNKNOWN\" token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ach-ytfQ9ITI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "    if word in punctuations:\n",
        "        sentence_words.remove(word)\n",
        "\n",
        "sentence_words\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ikozPM_5Z7"
      },
      "source": [
        "Task 2 - Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X4hB7Rt_4G8"
      },
      "outputs": [],
      "source": [
        "# 0 is negative and 1 is positive\n",
        "class NBC:\n",
        "    def __init__(self):\n",
        "        # Create an instance of the CountVectorizer class\n",
        "        self.vectorizer = CountVectorizer(strip_accents = 'unicode',        # Clean the data\n",
        "                                 stop_words = 'english', \n",
        "                                 lowercase = True, \n",
        "                                 max_df = 0.5, #May want to tweak these last two\n",
        "                                 min_df = 10)\n",
        "        self.prior = [0,0] # positive, negative                            # Initialize Prior\n",
        "        self.likelihood = [{},{}] # positive, negative                     # Initialize Liklihood\n",
        "\n",
        "    # this function is only for classifying positive and negative sentiment.\n",
        "    def fit(self,X,Y):\n",
        "        for j in range(1000,X.shape[0]+1,1000):\n",
        "          x = self.vectorizer.fit_transform(X[j-1000:j])\n",
        "          x = np.array(x.toarray())\n",
        "          positive_values = np.where(Y[j-1000:j] == 1, 1, 0)                # Convert Positive Values == 1   \n",
        "          negative_values = np.where(Y[j-1000:j] == 0, 1, 0)                # Convert Negative Values == 1 \n",
        "          self.prior[0] += (sum(positive_values)/x.shape[0])                \n",
        "          self.prior[1] += ((x.shape[0]-sum(positive_values))/x.shape[0])   \n",
        "          keys_to_pos = self.vectorizer.vocabulary_                         # Get dictionary (word : word position) in Tokenizer \n",
        "          keys = keys_to_pos.keys()                                         # Get words in dictionary \n",
        "          for key in keys:\n",
        "              i = keys_to_pos[key]\n",
        "              positive = sum(x[:,i].T * positive_values)\n",
        "              negative = sum(x[:,i].T * negative_values)\n",
        "              feature_name = key                  \n",
        "              if feature_name not in self.likelihood[0]:                    \n",
        "                self.likelihood[0][feature_name] = 0\n",
        "\n",
        "              if feature_name not in self.likelihood[1]:\n",
        "                self.likelihood[1][feature_name] = 0\n",
        "\n",
        "              self.likelihood[0][feature_name] += positive\n",
        "              self.likelihood[1][feature_name] += negative\n",
        "\n",
        "        # laplace smoothing\n",
        "        total_count = X.shape[0] + 2\n",
        "\n",
        "        for key in self.likelihood[0].keys():                              # Find likelihood by determining if word is a positive or negative sentiment\n",
        "            self.likelihood[0][key] += 1\n",
        "            self.likelihood[1][key] += 1\n",
        "            self.likelihood[0][key] /= total_count                         # Determine probablility for likelihoods\n",
        "            self.likelihood[1][key] /= total_count\n",
        "        \n",
        "        self.prior[0] /= total_count                                       # Determine probablility for priors\n",
        "        self.prior[1] /= total_count\n",
        "\n",
        "\n",
        "#     # this function is only for classifying positive and negative sentiment.\n",
        "#     def fit(self,X,Y):\n",
        "#         x = self.vectorizer.fit_transform(X)\n",
        "#         x = np.array(x.toarray())\n",
        "#         positive_values = np.where(Y == 1, 1, 0)         \n",
        "#         negative_values = np.where(Y == 0, 1, 0) \n",
        "#         self.prior.append(sum(positive_values)/x.shape[0])\n",
        "#         self.prior.append((x.shape[0]-sum(positive_values))/x.shape[0])\n",
        "#         for i in range(x.shape[1]):\n",
        "#             # adding laplace smoothing\n",
        "#             total_count = sum(x[:,i].T)\n",
        "#             positive = sum(x[:,i].T * positive_values) \n",
        "#             negative = sum(x[:,i].T * negative_values)\n",
        "#             feature_name = self.vectorizer.get_feature_names()[i]\n",
        "#             if positive > 0:\n",
        "#                 self.likelihood[0][feature_name] = positive/total_count\n",
        "#             if negative > 0:\n",
        "#                 self.likelihood[1][feature_name] = negative/total_count\n",
        "        \n",
        "    def predict(self,X):\n",
        "        predictions = []\n",
        "        for j in range(1000, X.shape[0]+1, 1000):\n",
        "            x = self.vectorizer.fit_transform(X[j-1000:j])\n",
        "            x = np.array(x.toarray())\n",
        "            keys_to_pos = self.vectorizer.vocabulary_\n",
        "            keys = keys_to_pos.keys()\n",
        "            for i in range(x.shape[0]):\n",
        "                \n",
        "                positive_prediction = self.prior[0]\n",
        "                for key in keys:\n",
        "                    j = keys_to_pos[key]\n",
        "                    if x[i,j] != 0 and key in self.likelihood[0]:\n",
        "                        positive_prediction *= self.likelihood[0][key] * x[i,j]\n",
        "                    \n",
        "                negative_prediction = self.prior[1]\n",
        "                for key in keys:\n",
        "                    j = keys_to_pos[key]\n",
        "                    if x[i,j] != 0 and key in self.likelihood[1]:\n",
        "                        negative_prediction *= self.likelihood[1][key] * x[i,j]\n",
        "                    \n",
        "                if positive_prediction >= negative_prediction:\n",
        "                    predictions.append(1)\n",
        "                else:\n",
        "                    predictions.append(0)\n",
        "                \n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def evaluate_accuracy(self, true_y, pred_y):\n",
        "        return sum(np.where(true_y == pred_y,1,0))/ true_y.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-V-mx-xGtD0"
      },
      "outputs": [],
      "source": [
        "#Train the model and get the test accuracy\n",
        "NaiveBayesClassifier = NBC()\n",
        "NaiveBayesClassifier.fit(rawTrain_x, rawTrain_y)\n",
        "predictions = NaiveBayesClassifier.predict(rawTest_x)\n",
        "print(f\"Naive Bayes Classifier Accuracy = {NaiveBayesClassifier.evaluate_accuracy(rawTest_y, predictions)*100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix_values(predicted, true):\n",
        "  TP = np.sum(np.where(np.where( predicted == 1 , 1, -1) == true, 1, 0))\n",
        "  FP = np.sum(np.where(np.where( predicted == 1 , 1, -1) == np.where( true == 0 , 1, 0), 1, 0))\n",
        "  TN = np.sum(np.where(np.where( predicted == 0 , 1, -1) == np.where( true == 0 , 1, 0), 1, 0))\n",
        "  FN = np.sum(np.where(np.where( predicted == 0 , 1, -1) == np.where( true == 1 , 1, 0), 1, 0))\n",
        "  return TP, FP, TN, FN\n",
        "\n",
        "TP, FP, TN, FN = confusion_matrix_values(predictions, rawTest_y)\n",
        "print(\"TP: \",TP, \"\\tFP: \", FP, \"\\tTN: \", TN, \"\\tFN: \", FN)"
      ],
      "metadata": {
        "id": "rhY37Q7VLV_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the train accuracy\n",
        "NaiveBayesClassifier = NBC()\n",
        "NaiveBayesClassifier.fit(rawTrain_x, rawTrain_y)\n",
        "predictions = NaiveBayesClassifier.predict(rawTrain_x)\n",
        "print(f\"Naive Bayes Classifier Accuracy = {NaiveBayesClassifier.evaluate_accuracy(rawTrain_y, predictions)*100}%\")\n",
        "TP, FP, TN, FN = confusion_matrix_values(predictions, rawTrain_y)\n",
        "print(\"TP: \",TP, \"\\tFP: \", FP, \"\\tTN: \", TN, \"\\tFN: \", FN)"
      ],
      "metadata": {
        "id": "SGe6BeWC8O37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z4PtgoTeN9Z"
      },
      "source": [
        "Logistic Regression Model using Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_PxTtsTeNjo"
      },
      "outputs": [],
      "source": [
        "#Trying out Logistic Regression on this problem. \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Convert the text to a bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(rawTrain_x)\n",
        "X_test = vectorizer.transform(rawTest_x)\n",
        "\n",
        "# Train the logistic regression model\n",
        "LRSAGModel = LogisticRegression(penalty='l2', solver='saga', verbose=False)\n",
        "LRSAGModel.fit(X_train, rawTrain_y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "LRSAG_predTest = LRSAGModel.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier on test\n",
        "accuracy = accuracy_score(rawTest_y, LRSAG_predTest)\n",
        "print(\"Logisitc Regression SAG Accuracy: \", accuracy * 100 , \"%\")\n",
        "\n",
        "# Make predictions on the train set\n",
        "LRSAG_pred = LRSAGModel.predict(X_train)\n",
        "\n",
        "# Evaluate the accuracy of the classifier on train\n",
        "accuracy = accuracy_score(rawTrain_y, LRSAG_pred)\n",
        "print(\"Logisitc Regression SAG Accuracy: \", accuracy * 100 , \"%\")\n",
        "\n",
        "#Try again with the SAGA model\n",
        "LRSAGAModel = LogisticRegression(penalty='l2', solver='saga', verbose=False)\n",
        "LRSAGAModel.fit(X_train, rawTrain_y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "LRSAGA_predTest = LRSAGAModel.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(rawTest_y, LRSAGA_predTest)\n",
        "print(\"Logisitc Regression SAGA Accuracy: \", accuracy * 100 , \"%\")\n",
        "\n",
        "# Make predictions on the train set\n",
        "LRSAGA_pred = LRSAGAModel.predict(X_train)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(rawTrain_y, LRSAGA_pred)\n",
        "print(\"Logisitc Regression SAGA Accuracy: \", accuracy * 100 , \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_confusion_matrix_values(cm):\n",
        "    return(cm[0][0], cm[0][1], cm[1][0], cm[1][1])\n",
        "\n",
        "confMatrixSAG = confusion_matrix(rawTest_y, LRSAG_predTest)\n",
        "TP, FP, FN, TN = get_confusion_matrix_values(confMatrixSAG)\n",
        "print(\"TP: \",TP, \"\\tFP: \", FP, \"\\tTN: \", TN, \"\\tFN: \", FN)\n",
        "confMatrixSAGA = confusion_matrix(rawTest_y, LRSAGA_predTest)\n",
        "TP, FP, FN, TN = get_confusion_matrix_values(confMatrixSAGA)\n",
        "print(\"TP: \",TP, \"\\tFP: \", FP, \"\\tTN: \", TN, \"\\tFN: \", FN)"
      ],
      "metadata": {
        "id": "dEQtzSuuQd5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP-rAiOdiFTO"
      },
      "source": [
        "Now we can test an SVM model, followed by a modified SVM model, NBSVM, which is described by Wang and Manning's paper. Uses Sklearn as above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KVbHwbvj2lA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "SVMModel = LinearSVC(C=0.5, random_state=42)\n",
        "SVMModel.fit(X_train, rawTrain_y)\n",
        "\n",
        "SVM_pred = SVMModel.predict(X_train)\n",
        "print(\"SVM Model Train Accuracy: \" , accuracy_score(rawTrain_y, SVM_pred) * 100 , \"%\")\n",
        "\n",
        "SVM_pred = SVMModel.predict(X_test)\n",
        "print(\"SVM Model Test Accuracy: \" , accuracy_score(rawTest_y, SVM_pred) * 100 , \"%\")\n",
        "\n",
        "confMatrixSVM = confusion_matrix(rawTest_y, SVM_pred)\n",
        "TP, FP, FN, TN = get_confusion_matrix_values(confMatrixSVM)\n",
        "print(\"TP: \",TP, \"\\tFP: \", FP, \"\\tTN: \", TN, \"\\tFN: \", FN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAue_SompjBL"
      },
      "source": [
        "Now for the NBSVM. Inspired by this tutorial: https://medium.com/@asmaiya/a-neural-implementation-of-nbsvm-in-keras-d4ef8c96cb7c which is based on the paper here: https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsXg--DZpRKY"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers import Input, Embedding, Flatten, dot\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "#NBSVM needs sequences of word IDs to work (just like we saw in class with the animal example). Do that here.\n",
        "def matrixToWordIds(matrix, maxlen):\n",
        "    x = []\n",
        "    result = []\n",
        "    for index, row in enumerate(matrix):\n",
        "        seq = []\n",
        "        indices = (row.indices + 1).astype(np.int64) #Ensure int conversion or get errors\n",
        "        np.append(result, len(indices))\n",
        "        data = (row.data).astype(np.int64) \n",
        "        count_dict = dict(zip(indices, data)) #Use zip to quickly make a dictionary\n",
        "        for k,v in count_dict.items():\n",
        "            seq.extend([k]*v)\n",
        "        num_words = len(seq)\n",
        "        result.append(num_words)\n",
        "        # Pad up to the max length with zeroes\n",
        "        if num_words < maxlen: \n",
        "            seq = np.pad(seq, (maxlen - num_words, 0),    \n",
        "                         mode='constant')\n",
        "        # Truncate down to max length\n",
        "        else:                  \n",
        "            seq = seq[-maxlen:]\n",
        "        x.append(seq)\n",
        "    result = np.array(result)\n",
        "    #print('Sequence stats: Avg: ', result.mean(), '\\tMax: ',result.max(), '\\tMin: ', result.min())\n",
        "    return np.array(x) #Better to return as nparray.\n",
        "\n",
        "maxlen = 2000\n",
        "x_train_NBSVM = matrixToWordIds(X_train, maxlen)\n",
        "x_test_NBSVM = matrixToWordIds(X_test, maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WV7_Iyrj66_"
      },
      "outputs": [],
      "source": [
        "#Need to get NB Log-count ratios. Use matrix form, not sequence form.\n",
        "def getRatios(matrix, y, yi):\n",
        "    p = matrix[y==yi].sum(0)\n",
        "    return (p+1) / ((y==yi).sum()+1)\n",
        "\n",
        "NBLogCountRatios = np.log(getRatios(X_train, rawTrain_y, 1)/getRatios(X_train, rawTrain_y, 0))\n",
        "NBLogCountRatios = np.squeeze(np.asarray(NBLogCountRatios))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHUtb3o6mbZj"
      },
      "outputs": [],
      "source": [
        "#Now we create the model. We'll need to get the amount of words in our vectorizer's dictionary.\n",
        "numWords = len([v for k,v in vectorizer.vocabulary_.items()]) + 1\n",
        "\n",
        "# Create the first embedding matrix which holds NB LogCount ratios\n",
        "embedding_matrix = np.zeros((numWords, 1))\n",
        "for i in range(1, numWords): # skip 0, the padding value\n",
        "  embedding_matrix[i] = NBLogCountRatios[i-1]\n",
        "# Setup the model and its parameters\n",
        "inp = Input(shape=(maxlen,))\n",
        "r = Embedding(numWords, 1, input_length=maxlen, weights=[embedding_matrix], trainable=False)(inp)\n",
        "x = Embedding(numWords, 1, input_length=maxlen, embeddings_initializer='glorot_normal')(inp) #The second embedded matrix which is for the learned weights as the model runs\n",
        "x = dot([r,x], axes=1)\n",
        "x = Flatten()(x)\n",
        "x = Activation('sigmoid')(x)\n",
        "NBSVMModel = Model(inputs=inp, outputs=x)\n",
        "NBSVMModel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUraTYwjn78p"
      },
      "outputs": [],
      "source": [
        "NBSVMModel.fit(x_train_NBSVM, rawTrain_y,batch_size=32,epochs=3, validation_data=(x_test_NBSVM, rawTest_y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NBSVM_pred= NBSVMModel.predict(x_test_NBSVM)\n",
        "confMatrixNBSVM = confusion_matrix(rawTest_y, np.rint(NBSVM_pred))\n",
        "TP, FP, FN, TN = get_confusion_matrix_values(confMatrixNBSVM)\n",
        "print(\"TP: \",TP, \"\\tFP: \", FP, \"\\tTN: \", TN, \"\\tFN: \", FN)"
      ],
      "metadata": {
        "id": "Msw8YpF9Ss1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data for transformer models. Note: For the below code and models, the intention is to run it on Kaggle with GPU acceleration due to memory constraints."
      ],
      "metadata": {
        "id": "bjVEiqMuZBBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q simpletransformers\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "import sklearn\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "cuda_available = torch.cuda.is_available()\n",
        "print(cuda_available)\n",
        "pandas_train = load_dataset('imdb',split='train')\n",
        "pandas_train.rename_column('label', 'labels')\n",
        "pandas_train = pd.DataFrame(pandas_train)\n",
        "\n",
        "pandas_test = load_dataset('imdb',split='test')\n",
        "pandas_test.rename_column('label', 'labels')\n",
        "pandas_test = pd.DataFrame(pandas_test)"
      ],
      "metadata": {
        "id": "-OXZrrRZZDZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_train"
      ],
      "metadata": {
        "id": "KL849FSSZUgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zkzmjm_4MgP"
      },
      "source": [
        "BERT Model using simpletransformers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pretrained BERT transformer model\n",
        "BERTModel = ClassificationModel('bert', 'bert-base-cased', use_cuda=True, num_labels=2, args={\n",
        "    'reprocess_input_data': False,\n",
        "    'overwrite_output_dir': True,\n",
        "    'sliding_window': True,\n",
        "    'max_seq_length': 64,\n",
        "    'num_train_epochs': 1,\n",
        "    'learning_rate': 0.00001,\n",
        "    'weight_decay': 0.01,\n",
        "    'train_batch_size': 128,\n",
        "    'fp16': True,\n",
        "    'output_dir': '/outputs/',\n",
        "}) \n",
        "\n",
        "#Train and evaluate the model\n",
        "BERTModel.train_model(pandas_train)"
      ],
      "metadata": {
        "id": "sEfyU_HTWzsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultBertTrain, output, badPredictions = BERTModel.eval_model(pandas_train, acc=sklearn.metrics.accuracy_score)\n",
        "print(\"Accuracy of BERT model on train: \" , resultBertTrain, \"%\")"
      ],
      "metadata": {
        "id": "YgFgGWAwaXg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultBertTest, output, badPredictions = BERTModel.eval_model(pandas_test, acc=sklearn.metrics.accuracy_score)\n",
        "print(\"Accuracy of BERT model on test: \" , resultBertTest, \"%\")"
      ],
      "metadata": {
        "id": "rh8Lmb1WaXvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the attention matrix results. Loaded from files saved previously (when running on Kaggle). We trained using Simpletransformers, but use the weights trained there in a tensorflow model to access its output_attentions feature. So, we have to re-load and clean the data a bit."
      ],
      "metadata": {
        "id": "mXtIACFPW0Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/outputs/\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"/outputs/\", output_attentions=True)"
      ],
      "metadata": {
        "id": "AovZGaepW-zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "data = tfds.load('imdb_reviews', as_supervised=True)"
      ],
      "metadata": {
        "id": "_H-JuTQdaluC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(sentence):\n",
        "    sentence = sentence.replace(\"\\\"\",\"\\'\").replace(\"\\'\",\"'\").replace('\\\\',\"\").replace(\"\\\\\",\"\").replace(\"xc2x96\",\"\").replace(\"xc2x97\",\"\").replace(\"xc2x85\",\"\").replace(\"xc2x9\",\"\").replace(\"xc2xa\",\"\").replace(\"xc2xa0\",\"\").replace(\"xc2xb4\",\"\").replace(\"xc2x91\",\"\").replace(\"xc2x84\",\"\").replace(\"xc3xa9\",\"\").replace(\"<br />\", \"\")[1:-1]\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "QtyvyNO7aoy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "def write_attention(filename, tensor):\n",
        "    # Open the CSV file for writing\n",
        "    with open(f\"/kaggle/working/{filename}\", 'w', newline='') as csvfile:\n",
        "        # Create a CSV writer object\n",
        "        writer = csv.writer(csvfile)\n",
        "    \n",
        "        # Write each row of the tensor as a separate row in the CSV file\n",
        "        for row in tensor:\n",
        "            writer.writerow(row.tolist())"
      ],
      "metadata": {
        "id": "HVd81_NCarMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = [], []\n",
        "test_x, test_y = [], []\n",
        "\n",
        "train_x_len = {}\n",
        "test_x_len = {}\n",
        "\n",
        "for line in data[\"test\"]:\n",
        "    sentence, val = clean_data(str(line[0].numpy())), int(line[1].numpy())\n",
        "    length = len(sentence.split())\n",
        "    if length <= 500:\n",
        "        test_x.append(sentence)\n",
        "        test_y.append(val)\n",
        "\n",
        "for line in data[\"train\"]:\n",
        "    sentence, val = clean_data(str(line[0].numpy())), int(line[1].numpy())\n",
        "    length = len(sentence.split())\n",
        "    if length <= 500:\n",
        "        train_x.append(sentence)\n",
        "        train_y.append(val)\n",
        "\n",
        "\n",
        "test_x1 = test_x[:12500]\n",
        "test_y1 = test_y[:12500]\n",
        "train_x1 = train_x[:5000]\n",
        "train_y1 = train_y[:5000]"
      ],
      "metadata": {
        "id": "8_85DZJ4bSP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy measuring functions. Can be ignored."
      ],
      "metadata": {
        "id": "j--6kOoobeof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the sentences. This section is just to test accuracy, it can be safely ignored.\n",
        "outputs = []\n",
        "\n",
        "for i in range(8000): #set to len(test_x) if you want full set\n",
        "    sentence = test_x[i]\n",
        "    inputs = tokenizer(sentence, add_special_tokens=True, return_tensors='pt', padding=True, truncation=True) #sentences[i]\n",
        "    batch_outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])[0]\n",
        "    predicted_classes = torch.argmax(batch_outputs, dim=1)\n",
        "    for value in predicted_classes:\n",
        "        outputs.append(int(value.numpy()))\n",
        "    if (i+1)%1000 ==0:\n",
        "        print(i+1)\n",
        "\n",
        "print(len(outputs))\n",
        "print(f\"BERT accuracy = {sum(np.where(np.array(outputs) == np.array(test_y),1,0))/len(outputs) * 100}\")"
      ],
      "metadata": {
        "id": "cm5n_grIbTco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "true_not_done, false_not_done = True, True\n",
        "for pred, true in zip(np.array(outputs),np.array(test_y)[:5386]):\n",
        "    if pred == true and true_not_done:\n",
        "        print(i)\n",
        "        true_not_done = False\n",
        "    elif pred != true and false_not_done:\n",
        "        print(i)\n",
        "        false_not_done = False\n",
        "    i += 1\n",
        "    if false_not_done == False and true_not_done == False:\n",
        "        break"
      ],
      "metadata": {
        "id": "EspYvXNUbaKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to save the pretrained model. For some reason, doesn't work."
      ],
      "metadata": {
        "id": "2aOkC6iIboSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"kaggle/working/pretrainedmodel/\"\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "model.save_pretrained(save_dir)"
      ],
      "metadata": {
        "id": "Z2kQ5dwFbizi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can get our attention. We test on two sentences, one which is correctly classifies and one which is wrong."
      ],
      "metadata": {
        "id": "rWKlogrGbzQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'This was a great movie! Matt Damon was in it, he was terrific.'\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "outputs = model(**inputs)\n",
        "predictions = outputs.logits.argmax(dim=1)\n",
        "\n",
        "# Print the predicted label\n",
        "print(predictions.item(), 1)\n",
        "\n",
        "attentions = outputs.attentions\n",
        "write_attention(\"attention_Correct_last_layer.csv\", attentions[-1][-1][-1])\n",
        "write_attention(\"attention_Correct_first_layer.csv\", attentions[0][0][0])\n",
        "\n",
        "def write_tokens(filename, tokensList):\n",
        "    # Open the CSV file for writing\n",
        "    with open(f\"/kaggle/working/{filename}\", 'w', newline='') as csvfile:\n",
        "        # Create a CSV writer object\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(tokensList)\n",
        "        \n",
        "tokens.append('SEP')\n",
        "tokens.insert(0, 'CLS')\n",
        "print(tokens)\n",
        "\n",
        "write_tokens(\"correctTokens.csv\", tokens)"
      ],
      "metadata": {
        "id": "esG5f4v_bvcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'This was a wonderful movie! But it has problems. The lighting, props and sets were bad, terrible and horrible. I still like it.'\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "outputs = model(**inputs)\n",
        "predictions = outputs.logits.argmax(dim=1)\n",
        "\n",
        "# Print the predicted label\n",
        "print(predictions.item(), 1)\n",
        "\n",
        "attentions = outputs.attentions\n",
        "write_attention(\"attention_Wrong_last_layer.csv\", attentions[-1][-1][-1])\n",
        "write_attention(\"attention_Wrong_first_layer.csv\", attentions[0][0][0])\n",
        "\n",
        "tokens.append('SEP')\n",
        "tokens.insert(0, 'CLS')\n",
        "\n",
        "write_tokens(\"wrongTokens.csv\", tokens)"
      ],
      "metadata": {
        "id": "THdk-KZqcLwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we had to run some parts on kaggle, we load from files (we just consoliated everything into this one notebook for simplicity). So this is not necessarily necessary if all running in one file."
      ],
      "metadata": {
        "id": "M48REjIRcO_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tokens_correct = pd.read_csv(\"/kaggle/input/imdb-attention-max-bert/correctTokens.csv\")\n",
        "correct_tokens = tokens_correct.columns.values.tolist()\n",
        "tokens_wrong = pd.read_csv(\"/kaggle/input/imdb-attention-max-bert/wrongTokens.csv\")\n",
        "wrong_tokens = tokens_wrong.columns.values.tolist()\n",
        "print(correct_tokens)\n",
        "print(wrong_tokens)"
      ],
      "metadata": {
        "id": "phLtMVmUcaRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename_attention_correct_fl = \"/kaggle/input/imdb-attention-max-bert/attention_Correct_first_layer.csv\"\n",
        "df_correct_fl = pd.read_csv(filename_attention_correct_fl)\n",
        "\n",
        "plt.subplots(figsize=(8, 6))\n",
        "array_attention_correct_fl = df_correct_fl.to_numpy()\n",
        "s1 = sns.heatmap(array_attention_correct_fl)\n",
        "s1.set_xticks(np.arange(len(correct_tokens)), labels = correct_tokens)\n",
        "s1.set_yticks(np.arange(len(correct_tokens)), labels = correct_tokens)\n",
        "plt.title(\"First Layer Attention Matrix of Correctly Classified Sentence\")\n",
        "plt.xticks(rotation=90) \n",
        "plt.yticks(rotation=0) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iVUM_WR1ciKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename_attention_correct_ll = \"/kaggle/input/imdb-attention-max-bert/attention_Correct_last_layer.csv\"\n",
        "df_correct_ll = pd.read_csv(filename_attention_correct_ll)\n",
        "\n",
        "plt.subplots(figsize=(8, 6))\n",
        "array_attention_correct_ll = df_correct_ll.to_numpy()\n",
        "s2 = sns.heatmap(array_attention_correct_ll)\n",
        "s2.set_xticks(np.arange(len(correct_tokens)), labels = correct_tokens)\n",
        "s2.set_yticks(np.arange(len(correct_tokens)), labels = correct_tokens)\n",
        "plt.title(\"Last Layer Attention Matrix of Correctly Classified Sentence\")\n",
        "plt.xticks(rotation=90) \n",
        "plt.yticks(rotation=0) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZQP046jdckgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename_attention_wrong_fl = \"/kaggle/input/imdb-attention-max-bert/attention_Wrong_first_layer.csv\"\n",
        "df_wrong_fl = pd.read_csv(filename_attention_wrong_fl)\n",
        "array_attention_wrong_fl = df_wrong_fl.to_numpy()\n",
        "\n",
        "plt.subplots(figsize=(8, 6))\n",
        "s3 = sns.heatmap(array_attention_wrong_fl)\n",
        "s3.set_xticks(np.arange(len(wrong_tokens)), labels = wrong_tokens)\n",
        "s3.set_yticks(np.arange(len(wrong_tokens)), labels = wrong_tokens)\n",
        "plt.title(\"First Layer Attention Matrix of Incorrect Classified Sentence\")\n",
        "plt.xticks(rotation=90) \n",
        "plt.yticks(rotation=0) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9HAn_wejcmJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename_attention_wrong_ll = \"/kaggle/input/imdb-attention-max-bert/attention_Wrong_last_layer.csv\"\n",
        "df_wrong_ll = pd.read_csv(filename_attention_wrong_ll)\n",
        "array_attention_wrong_ll = df_wrong_ll.to_numpy()\n",
        "\n",
        "plt.subplots(figsize=(8, 6))\n",
        "s4 = sns.heatmap(array_attention_wrong_ll)\n",
        "s4.set_xticks(np.arange(len(wrong_tokens)), labels = wrong_tokens)\n",
        "s4.set_yticks(np.arange(len(wrong_tokens)), labels = wrong_tokens)\n",
        "plt.title(\"Last Layer Attention Matrix of Incorrect Classified Sentence\")\n",
        "plt.xticks(rotation=90) \n",
        "plt.yticks(rotation=0) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wjUEWD79cqPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights = sns.load_dataset(\"flights\")\n",
        "flights = flights.pivot(\"month\", \"year\", \"passengers\")\n",
        "f,(ax1,ax2,ax3, axcb) = plt.subplots(,2, \n",
        "            gridspec_kw={'width_ratios':[1,1,1,0.08]})\n",
        "ax1.get_shared_y_axes().join(ax2,ax3)\n",
        "g1 = sns.heatmap(array_attention_correct_fl,cbar=False,ax=ax1)\n",
        "g1.set_ylabel('')\n",
        "g1.set_xlabel('')\n",
        "g2 = sns.heatmap(array_attention_correct_fl)\n",
        "g2.set_ylabel('')\n",
        "g2.set_xlabel('')\n",
        "g2.set_yticks([])\n",
        "g3 = sns.heatmap(array_attention_correct_fl, cbar=False,ax=ax2))\n",
        "g3.set_ylabel('')\n",
        "g3.set_xlabel('')\n",
        "g3.set_yticks([])\n",
        "\n",
        "# may be needed to rotate the ticklabels correctly:\n",
        "for ax in [g1,g2,g3]:\n",
        "    tl = ax.get_xticklabels()\n",
        "    ax.set_xticklabels(tl, rotation=90)\n",
        "    tly = ax.get_yticklabels()\n",
        "    ax.set_yticklabels(tly, rotation=0)"
      ],
      "metadata": {
        "id": "XQ4eRYGBc18Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(11, 9),layout='constrained',\n",
        "            gridspec_kw={'width_ratios':[1,1]})\n",
        "f.suptitle(\"Attention Matrices of Correctly Classified Sentence\", y=0.8)\n",
        "ax1.get_shared_y_axes().join(ax2)\n",
        "g1 = sns.heatmap(array_attention_correct_fl,cbar=False,ax=ax1)\n",
        "g1.set_title('First Layer')\n",
        "g1.set_ylabel('')\n",
        "g1.set_xlabel('')\n",
        "g1.set_yticks(np.arange(len(correct_tokens)), labels = correct_tokens, rotation=0)\n",
        "g2 = sns.heatmap(array_attention_correct_ll,ax=ax2, cbar=True, cbar_kws={\"shrink\": .5})\n",
        "g2.set_title('Last Layer')\n",
        "g2.set_ylabel('')\n",
        "g2.set_xlabel('')\n",
        "g2.set_yticks([])\n",
        "\n",
        "# may be needed to rotate the ticklabels correctly:\n",
        "for ax in [g1,g2]:\n",
        "    ax.set_xticks(np.arange(len(correct_tokens)), labels = correct_tokens, rotation=90)\n",
        "    ax.set_aspect('equal')\n",
        "    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ttr3Ph7Ac4sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,(ax1,ax2) = plt.subplots(1,2, figsize=(11, 9),layout='constrained',\n",
        "            gridspec_kw={'width_ratios':[1,1]})\n",
        "f.suptitle(\"Attention Matrices of Incorrectly Classified Sentence\", y=0.8)\n",
        "ax1.get_shared_y_axes().join(ax2)\n",
        "g1 = sns.heatmap(array_attention_wrong_fl,cbar=False,ax=ax1)\n",
        "g1.set_title('First Layer')\n",
        "g1.set_ylabel('')\n",
        "g1.set_xlabel('')\n",
        "g1.set_yticks(np.arange(len(wrong_tokens)), labels = wrong_tokens, rotation=0)\n",
        "g2 = sns.heatmap(array_attention_wrong_ll,ax=ax2, cbar=True, cbar_kws={\"shrink\": .5})\n",
        "g2.set_title('Last Layer')\n",
        "g2.set_ylabel('')\n",
        "g2.set_xlabel('')\n",
        "g2.set_yticks([])\n",
        "\n",
        "# may be needed to rotate the ticklabels correctly:\n",
        "for ax in [g1,g2]:\n",
        "    ax.set_xticks(np.arange(len(wrong_tokens)), labels = wrong_tokens, rotation=90)\n",
        "    ax.set_aspect('equal')\n",
        "    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sDllHTIKc6Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALBERT Model (also using simpletransformers)"
      ],
      "metadata": {
        "id": "CONJEuETW_Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pretrained ALBERT transformer model\n",
        "ALBERTModel = ClassificationModel('albert', 'albert-base-v2', use_cuda=True, num_labels=2, args={\n",
        "    'reprocess_input_data': False,\n",
        "    'overwrite_output_dir': True,\n",
        "    'sliding_window': True,\n",
        "    'max_seq_length': 64,\n",
        "    'num_train_epochs': 1,\n",
        "    'learning_rate': 0.00001,\n",
        "    'weight_decay': 0.01,\n",
        "    'train_batch_size': 128,\n",
        "    'fp16': True,\n",
        "    'output_dir': '/outputs/',\n",
        "}) \n",
        "\n",
        "#Train and evalutae the model\n",
        "ALBERTModel.train_model(pandas_train)"
      ],
      "metadata": {
        "id": "HIvulhzvXAmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALBERTresult, ALBERToutput, ALBERTbadPredictions = ALBERTModel.eval_model(pandas_train, acc=sklearn.metrics.accuracy_score)\n",
        "print(\"Accuracy of ALBERT model on test: \" , ALBERTresult, \"%\")"
      ],
      "metadata": {
        "id": "cGJzpUqiZsKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALBERTresult, ALBERToutput, ALBERTbadPredictions = ALBERTModel.eval_model(pandas_test, acc=sklearn.metrics.accuracy_score)\n",
        "print(\"Accuracy of ALBERT model on test: \" , ALBERTresult, \"%\")"
      ],
      "metadata": {
        "id": "xvTIgB1HZsYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XLNet Model (also using simpletransformers)"
      ],
      "metadata": {
        "id": "BTahGuOoXAzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pretrained XLNet transformer model\n",
        "XLNetModel = ClassificationModel('xlnet', 'xlnet-base-cased', use_cuda=True, num_labels=2, args={\n",
        "    'reprocess_input_data': False,\n",
        "    'overwrite_output_dir': True,\n",
        "    'sliding_window': True,\n",
        "    'max_seq_length': 64,\n",
        "    'num_train_epochs': 1,\n",
        "    'learning_rate': 0.00001,\n",
        "    'weight_decay': 0.01,\n",
        "    'train_batch_size': 128,\n",
        "    'fp16': True,\n",
        "    'output_dir': '/outputs/',\n",
        "    'output_attention' : True\n",
        "}) \n",
        "\n",
        "#Train and evaluate the model\n",
        "XLNetModel.train_model(pandas_train)"
      ],
      "metadata": {
        "id": "g3llXnEFXCDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XLNetResult, XLNetOutput, XLNetBadPredictions = XLNetModel.eval_model(pandas_train, acc=sklearn.metrics.accuracy_score)\n",
        "print(\"Accuracy of XLNet model: \" , XLNetResult, \"%\")"
      ],
      "metadata": {
        "id": "3sw8Pz7bZfxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XLNetResult, XLNetOutput, XLNetBadPredictions = XLNetModel.eval_model(pandas_test, acc=sklearn.metrics.accuracy_score)\n",
        "print(\"Accuracy of XLNet model: \" , XLNetResult, \"%\")"
      ],
      "metadata": {
        "id": "VcNzaRYKZ_ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP model"
      ],
      "metadata": {
        "id": "VO6MBY_er446"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pickle \n",
        "import sys\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf \n",
        "import tensorflow.keras as k\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "z8cMr8GCsKWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "data = tfds.load('imdb_reviews', as_supervised=True)"
      ],
      "metadata": {
        "id": "wrkgz6f7sLZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(sentence):\n",
        "    sentence = sentence.replace(\"\\\"\",\"\\'\").replace(\"\\'\",\"'\").replace('\\\\',\"\").replace(\"\\\\\",\"\").replace(\"xc2x96\",\"\").replace(\"xc2x97\",\"\").replace(\"xc2x85\",\"\").replace(\"xc2x9\",\"\").replace(\"xc2xa\",\"\").replace(\"xc2xa0\",\"\").replace(\"xc2xb4\",\"\").replace(\"xc2x91\",\"\").replace(\"xc2x84\",\"\").replace(\"xc3xa9\",\"\").replace(\"<br />\", \"\")[1:-1]\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "bXZED80NsL9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = [], []\n",
        "test_x, test_y = [], []\n",
        "\n",
        "train_x_len = {}\n",
        "test_x_len = {}\n",
        "\n",
        "for line in data[\"test\"]:\n",
        "    sentence, val = clean_data(str(line[0].numpy())), int(line[1].numpy())\n",
        "    length = len(sentence.split())\n",
        "    if length <= 500:\n",
        "        test_x.append(sentence)\n",
        "        test_y.append(val)\n",
        "\n",
        "for line in data[\"train\"]:\n",
        "    sentence, val = clean_data(str(line[0].numpy())), int(line[1].numpy())\n",
        "    length = len(sentence.split())\n",
        "    if length <= 500:\n",
        "        train_x.append(sentence)\n",
        "        train_y.append(val)\n",
        "\n",
        "\n",
        "test_x1 = test_x[:15000]\n",
        "test_y1 = test_y[:150000]\n",
        "train_x1 = train_x[:15000]\n",
        "train_y1 = train_y[:15000]"
      ],
      "metadata": {
        "id": "CYStocLpsM6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_len = len(test_x1)\n",
        "test_len"
      ],
      "metadata": {
        "id": "RqA9GfQYsP5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "8NBZ9seTsQ0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "\n",
        "values = vectorizer.fit_transform(test_x1+train_x1)\n",
        "values = values.toarray()"
      ],
      "metadata": {
        "id": "j8Oze-nUsSnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer_len = len(values[0])\n",
        "input_layer_len"
      ],
      "metadata": {
        "id": "h1bmphWPsTsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(true_y, pred_y):\n",
        "    return sum(np.where(true_y == pred_y, 1, 0))/ true_y.shape[0]"
      ],
      "metadata": {
        "id": "qW2kHIv4sVOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer():\n",
        "    def __init__(self,x_size, y_size, reg=None, reg_strength=0.01):\n",
        "        self.w = np.random.normal(0 , 0.001, size = (y_size, x_size))\n",
        "        self.b = np.random.normal(0 , 0.001, size = (y_size,1))\n",
        "        self.dw = None\n",
        "        self.db = None\n",
        "        self.reg = reg\n",
        "        self.reg_strength = reg_strength\n",
        "        self.input = None    \n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        return (self.w @ x.T + self.b).T\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        self.dw = (self.input.T @ gradient).T \n",
        "        self.db = np.sum(gradient)\n",
        "        \n",
        "        # Regularization\n",
        "        if self.reg == \"L1\":\n",
        "            self.dw += self.reg_strength * np.abs(self.w)\n",
        "        elif self.reg == \"L2\":\n",
        "            self.dw += self.reg_strength * np.square(self.w)\n",
        "        \n",
        "        return gradient.dot(self.w)\n",
        "\n",
        "        \n",
        "class ReLULayer():\n",
        "    def __init__(self):\n",
        "        self.gradient = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
        "        return  np.maximum(0, x)\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        return gradient * self.gradient\n",
        "    \n",
        "\n",
        "class LeakyReLULayer():\n",
        "    def __init__(self, alpha=0.01):\n",
        "        self.alpha = alpha\n",
        "        self.gradient = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.gradient = np.where(x > 0, 1.0, self.alpha)\n",
        "        return  np.where(x > 0, x, x * self.alpha)\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        return gradient * self.gradient\n",
        "    \n",
        "class TanHLayer():\n",
        "    def __init__(self):\n",
        "        self.gradient = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.gradient = 1 - np.tanh(x)**2\n",
        "        return np.tanh(df)\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        return gradient * self.gradient\n",
        "    \n",
        "    \n",
        "class SoftmaxLayer():\n",
        "    def __init__(self):\n",
        "        self.probabilities = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        exps = np.exp(x)\n",
        "        probs = exps / np.sum(exps, axis=-1)[:, None]\n",
        "        self.probabilities = probs\n",
        "        return probs\n",
        "\n",
        "    def backward(self, target):\n",
        "        return self.probabilities - target\n",
        "    \n",
        "\n",
        "def activation_func(f):\n",
        "        if f == \"Relu\":\n",
        "            return ReLULayer()\n",
        "        elif f == \"LeakyReLU\":\n",
        "            return LeakyReLULayer()\n",
        "        elif f == \"Tanh\":\n",
        "            return TanHLayer()\n",
        "        \n",
        "class MLP:\n",
        "    def __init__(self, functions, layer_units, learning_rate, reg=None, reg_strength=0.01):\n",
        "        self.layers = []\n",
        "        self.learning_rate = learning_rate\n",
        "        if len(layer_units) != 0:\n",
        "            self.layers.append(LinearLayer(input_layer_len, layer_units[0],reg, reg_strength))\n",
        "            self.layers.append(activation_func(functions[0]))\n",
        "            for i in range(0,len(layer_units)-1):\n",
        "                self.layers.append(LinearLayer(layer_units[i], layer_units[i+1],reg, reg_strength))\n",
        "                self.layers.append(activation_func(functions[i+1]))\n",
        "            self.layers.append(LinearLayer(layer_units[-1], 2 ,reg, reg_strength))\n",
        "            self.layers.append(SoftmaxLayer())\n",
        "\n",
        "        self.loss_array = []\n",
        "        self.accuracy_array = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, target):\n",
        "        for layer in self.layers[::-1]:\n",
        "            target = layer.backward(target)\n",
        "\n",
        " \n",
        "    def fit(self, data_x, data_y, iterations):\n",
        "        labels = np.eye(2)[np.array(data_y)]\n",
        "        x = np.split(data_x, 200)\n",
        "        y = np.split(labels, 200)\n",
        "        n = 0\n",
        "        opt = GradientDescentOptimizer(self)\n",
        "        for i in tqdm(range(iterations)):\n",
        "            predictions = self.forward(x[n])\n",
        "            loss = -(y[n] * np.log(predictions)).sum(axis=-1).mean()\n",
        "            self.loss_array.append(loss)\n",
        "            self.backward(y[n])\n",
        "            opt.update()\n",
        "            \n",
        "            n=n+1\n",
        "            if n == 200:\n",
        "                n = 0\n",
        "        \n",
        "    def predict(self,data_x):\n",
        "        predictions = self.forward(data_x)\n",
        "        return np.argmax(predictions, axis = 1)\n",
        "\n",
        "        \n",
        "class GradientDescentOptimizer:\n",
        "    def __init__(self, mlp):\n",
        "        self.mlp = mlp\n",
        "        \n",
        "    def update(self):\n",
        "        for i in range(len(self.mlp.layers)-2,-1,-2):    \n",
        "            self.mlp.layers[i].b -= self.mlp.learning_rate * self.mlp.layers[i].db          \n",
        "            self.mlp.layers[i].w -= self.mlp.learning_rate * self.mlp.layers[i].dw"
      ],
      "metadata": {
        "id": "65x4Etttr4dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(values[test_len:]), len(train_y1))"
      ],
      "metadata": {
        "id": "DJH1N_SYsGL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLP([\"Relu\", \"Relu\"], [6860, 256], 0.00001)\n",
        "mlp.fit(values[test_len:], train_y1, 1000)"
      ],
      "metadata": {
        "id": "94x-4ga-sXyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = mlp.predict(values[:10000])\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "xrso49u7sY9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum(predictions == train_y1[:10000])/len(train_y1[:10000]) * 100)"
      ],
      "metadata": {
        "id": "tvLouIJTsZ7z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}